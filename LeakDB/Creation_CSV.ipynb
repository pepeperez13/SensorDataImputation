{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeakDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Load data from all scenarios\n",
    "import pickle\n",
    "\n",
    "# Load dictionary from pickle file\n",
    "with open(\"scenario_data.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "print(type(G)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n"
     ]
    }
   ],
   "source": [
    "# Check structure\n",
    "print(G['Scenario'].keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Scenario\n",
      "  Subfolder: 1\n",
      "    File: Labels.csv\n",
      "    File: Scenario-1_info.csv\n",
      "    File: Demands\n",
      "    File: Flows\n",
      "    File: Leaks\n",
      "    File: Pressures\n",
      "  Subfolder: 2\n",
      "    File: Labels.csv\n",
      "    File: Scenario-2_info.csv\n",
      "    File: Demands\n",
      "    File: Flows\n",
      "    File: Leaks\n",
      "    File: Pressures\n",
      "  Subfolder: 3\n",
      "    File: Labels.csv\n",
      "    File: Scenario-3_info.csv\n",
      "    File: Demands\n",
      "    File: Flows\n",
      "    File: Leaks\n",
      "    File: Pressures\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the available data\n",
    "for scenario, scenario_data in G.items():\n",
    "    print(f\"Scenario: {scenario}\")  # Prints the scenario number\n",
    "    \n",
    "    for subfolder, subfolder_data in scenario_data.items():\n",
    "        if (int(subfolder) < 4):\n",
    "            print(f\"  Subfolder: {subfolder}\")  # Prints the subfolder name\n",
    "\n",
    "            for file, df in subfolder_data.items():\n",
    "                    print(f\"    File: {file}\")  # Prints the CSV filename\n",
    "        else:\n",
    "             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Processes demands, pressures, flows, and leaks for a given scenario in `G`.\n",
    "def process_df_node_data(scenario_dict):\n",
    "\n",
    "    demands_dict = scenario_dict.get(\"Demands\", {})\n",
    "    pressures_dict = scenario_dict.get(\"Pressures\", {})\n",
    "    flows_dict = scenario_dict.get(\"Flows\", {})\n",
    "    #leaks_dict = scenario_dict.get(\"Leaks\", {})\n",
    "\n",
    "    # gather data associated with nodes\n",
    "\n",
    "    nodes_data = []\n",
    "    for file_name in sorted(demands_dict.keys()):\n",
    "        if file_name in pressures_dict:\n",
    "\n",
    "            d_df = demands_dict[file_name]\n",
    "            p_df = pressures_dict[file_name]\n",
    "\n",
    "            # Merge on common node identifier (assuming \"NodeID\" column)\n",
    "            merged_df = d_df.merge(p_df, on=\"Timestamp\", suffixes=(\"_Demand\", \"_Pressure\"))\n",
    "                \n",
    "            merged_df.rename(columns={\"Value_Demand\": \"Demand\", \"Value_Pressure\": \"Pressure\", \"Description\": \"Leak_Demand\"}, inplace=True)\n",
    "            merged_df['node_id'] = file_name.replace('.csv', '')\n",
    "            nodes_data.append(merged_df)\n",
    "\n",
    "    # Combine all time-step data for demands and pressures\n",
    "    nodes_data = pd.concat(nodes_data, ignore_index=True) if nodes_data else None\n",
    "\n",
    "    # Gather data associated with pipes (links)\n",
    "    links_data = []\n",
    "    for file_name in sorted(flows_dict):\n",
    "        f_df = flows_dict[file_name]\n",
    "        f_df['link_id'] = file_name.replace('.csv', '')\n",
    "        links_data.append(f_df)\n",
    "\n",
    "    # Combine all time-step data for flows\n",
    "    links_data = pd.concat(links_data, ignore_index=True) if links_data else None\n",
    "\n",
    "    # Return structured data\n",
    "    return nodes_data, links_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data from all scenarios in a single dictionary\n",
    "processed_scenarios = {}\n",
    "\n",
    "for scenario in G[\"Scenario\"]:\n",
    "    nodes_data, links_data = process_df_node_data(G[\"Scenario\"][scenario])\n",
    "    processed_scenarios[scenario] = {\"nodes\": nodes_data, \"links\": links_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed scenario 1\n",
      "Processed scenario 2\n",
      "Processed scenario 3\n",
      "Processed scenario 4\n",
      "Processed scenario 5\n",
      "Processed scenario 6\n",
      "Processed scenario 7\n",
      "Processed scenario 8\n",
      "Processed scenario 9\n",
      "Processed scenario 10\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to collect results\n",
    "all_scenarios = []\n",
    "\n",
    "# Iterate over the scenarios\n",
    "for scenario_number in processed_scenarios.keys():\n",
    "    # Extract nodes and links for each scenario\n",
    "    nodes_scenario = processed_scenarios[scenario_number]['nodes']\n",
    "    links_scenario = processed_scenarios[scenario_number]['links']\n",
    "    \n",
    "    # Create df with unique identifiers for each sensor measurement, regarding demands\n",
    "    demands_scenario = nodes_scenario.drop(columns=[\"Pressure\"])\n",
    "    demands_scenario[\"sensor_id\"] = demands_scenario[\"node_id\"] + \"_demand\"\n",
    "    demands_scenario = demands_scenario.drop(columns=\"node_id\").rename(columns={\"Demand\": \"measurement\"})\n",
    "    demands_scenario[\"sensor_type\"] = \"demand\"\n",
    "\n",
    "    # Create df with unique identifiers for each sensor measurement, regarding pressures\n",
    "    pressures_scenario = nodes_scenario.drop(columns=[\"Demand\"])\n",
    "    pressures_scenario[\"sensor_id\"] = pressures_scenario[\"node_id\"] + \"_pressure\"\n",
    "    pressures_scenario = pressures_scenario.drop(columns=\"node_id\").rename(columns={\"Pressure\": \"measurement\"})\n",
    "    pressures_scenario[\"sensor_type\"] = \"pressure\"\n",
    "\n",
    "    # Create df with unique identifiers for each sensor measurement, regarding flows\n",
    "    flows_scenario = links_scenario.copy()\n",
    "    flows_scenario[\"sensor_id\"] = links_scenario[\"link_id\"] + \"_flow\"\n",
    "    flows_scenario = flows_scenario.drop(columns=\"link_id\").rename(columns={\"Value\": \"measurement\"})\n",
    "    flows_scenario[\"sensor_type\"] = \"flow\"\n",
    "\n",
    "    # Combine all the sensor measurements into a final dataframe for this scenario\n",
    "    all_scenario = pd.concat([demands_scenario, pressures_scenario, flows_scenario], ignore_index=True)\n",
    "    \n",
    "    # Add a unique identifier for each sensor measurement\n",
    "    all_scenario['unique_id'] = all_scenario['sensor_id'] + '_' + all_scenario[\"Timestamp\"].astype(str)\n",
    "    \n",
    "    all_scenario.to_csv(f\"measurements_{scenario_number}_LeakDB.csv\")\n",
    "\n",
    "    # You can print or work with each scenario's dataframe (all_scenario) here\n",
    "    print(f\"Processed scenario {scenario_number}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
